{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Large/Nested Contest Analyze Data\n",
    "\n",
    "This notebook demonstrates how to load contest_analyze data that has:\n",
    "- Large nested JSON structures\n",
    "- Objects exceeding DuckDB's default 16MB limit\n",
    "- Deeply nested dictionaries and lists\n",
    "\n",
    "**Problem:** Contest analyze data has very large nested objects (~33MB+) that exceed DuckDB's default `maximum_object_size`.\n",
    "\n",
    "**Solution:** Configure DuckDB with larger limits and use memory-efficient queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup with Large Object Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Get Wasabi credentials\n",
    "wasabi_endpoint = os.getenv('WASABI_ENDPOINT', 's3.us-east-2.wasabisys.com')\n",
    "wasabi_access_key = os.getenv('WASABI_ACCESS_KEY')\n",
    "wasabi_secret_key = os.getenv('WASABI_SECRET_KEY')\n",
    "bucket_name = os.getenv('WASABI_BUCKET_NAME')\n",
    "\n",
    "# Create DuckDB connection\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Configure S3 settings\n",
    "con.execute(f\"\"\"\n",
    "    SET s3_endpoint='{wasabi_endpoint}';\n",
    "    SET s3_access_key_id='{wasabi_access_key}';\n",
    "    SET s3_secret_access_key='{wasabi_secret_key}';\n",
    "    SET s3_url_style='path';\n",
    "\"\"\")\n",
    "\n",
    "print(\"✓ DuckDB configured with S3 credentials\")\n",
    "print(f\"Endpoint: {wasabi_endpoint}\")\n",
    "print(f\"Bucket: {bucket_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update these to match your data\n",
    "SPORT = \"NFL\"  # e.g., \"NFL\", \"NBA\", etc.\n",
    "DATE = \"2025-10-02\"  # Format: YYYY-MM-DD\n",
    "GAME_TYPE = \"dk_single_game\"  # e.g., \"classic\", \"dk_single_game\", etc.\n",
    "\n",
    "# Maximum JSON object size (default is 16MB, we're setting to 64MB)\n",
    "MAX_OBJECT_SIZE = 67108864  # 64MB in bytes\n",
    "\n",
    "# Construct path\n",
    "contest_analyze_path = f\"s3://{bucket_name}/staging/{SPORT}/contest_analyze/{GAME_TYPE}/{DATE}/data.json.gz\"\n",
    "\n",
    "print(f\"Loading from: {contest_analyze_path}\")\n",
    "print(f\"Max object size: {MAX_OBJECT_SIZE / 1024 / 1024:.0f}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Error\n",
    "\n",
    "**Error you saw:**\n",
    "```\n",
    "InvalidInputException: \"maximum_object_size\" of 16777216 bytes exceeded\n",
    "while reading file (>33554428 bytes)\n",
    "```\n",
    "\n",
    "**What this means:**\n",
    "- Each JSON object in your file is ~33.5MB\n",
    "- DuckDB's default limit is 16MB\n",
    "- Your contest_analyze data has deeply nested structures\n",
    "\n",
    "**Why this happens:**\n",
    "- Contest analyze responses contain massive nested dicts\n",
    "- Each contest has detailed analysis data with many fields\n",
    "- Pandas saved these as-is (preserving structure)\n",
    "\n",
    "**The fix:**\n",
    "- Set `maximum_object_size` parameter in `read_json()`\n",
    "- Use memory-efficient queries (filter early, don't load all at once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Load Contest Analyze Data (with large object support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with increased maximum_object_size\n",
    "result = con.execute(f\"\"\"\n",
    "    SELECT * \n",
    "    FROM read_json(\n",
    "        '{contest_analyze_path}', \n",
    "        format='newline_delimited',\n",
    "        compression='gzip',\n",
    "        maximum_object_size={MAX_OBJECT_SIZE}\n",
    "    )\n",
    "    LIMIT 5\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"✓ Successfully loaded {len(result)} rows\")\n",
    "print(f\"\\nColumns: {list(result.columns)}\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total records\n",
    "count = con.execute(f\"\"\"\n",
    "    SELECT COUNT(*) as total_records\n",
    "    FROM read_json(\n",
    "        '{contest_analyze_path}',\n",
    "        format='newline_delimited',\n",
    "        compression='gzip',\n",
    "        maximum_object_size={MAX_OBJECT_SIZE}\n",
    "    )\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"Total records: {count['total_records'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Inspect Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get schema\n",
    "schema = con.execute(f\"\"\"\n",
    "    DESCRIBE \n",
    "    SELECT * FROM read_json(\n",
    "        '{contest_analyze_path}',\n",
    "        format='newline_delimited',\n",
    "        compression='gzip',\n",
    "        maximum_object_size={MAX_OBJECT_SIZE}\n",
    "    )\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"Schema of contest_analyze data:\")\n",
    "print(\"=\" * 70)\n",
    "for _, row in schema.iterrows():\n",
    "    print(f\"{row['column_name']:40s} : {row['column_type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine one record in detail\n",
    "sample = con.execute(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM read_json(\n",
    "        '{contest_analyze_path}',\n",
    "        format='newline_delimited',\n",
    "        compression='gzip',\n",
    "        maximum_object_size={MAX_OBJECT_SIZE}\n",
    "    )\n",
    "    LIMIT 1\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"\\nDetailed structure of one record:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for col in sample.columns:\n",
    "    value = sample[col].iloc[0]\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Type: {type(value).__name__}\")\n",
    "    \n",
    "    if isinstance(value, dict):\n",
    "        keys = list(value.keys())\n",
    "        print(f\"  Dict with {len(keys)} keys\")\n",
    "        print(f\"  First 10 keys: {keys[:10]}\")\n",
    "        if len(keys) > 10:\n",
    "            print(f\"  ... and {len(keys) - 10} more keys\")\n",
    "    elif isinstance(value, list):\n",
    "        print(f\"  List with {len(value)} items\")\n",
    "        if len(value) > 0:\n",
    "            print(f\"  First item type: {type(value[0]).__name__}\")\n",
    "            if isinstance(value[0], dict):\n",
    "                print(f\"  First item keys: {list(value[0].keys())[:5]}\")\n",
    "    elif isinstance(value, str) and len(value) > 100:\n",
    "        print(f\"  String (length: {len(value)} chars)\")\n",
    "        print(f\"  Preview: {value[:100]}...\")\n",
    "    else:\n",
    "        print(f\"  Value: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Memory-Efficient Queries\n",
    "\n",
    "**IMPORTANT:** Don't load all data at once! Use filters and aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get just the columns you need (not all columns)\n",
    "# Adjust column names based on your actual schema\n",
    "\n",
    "# selected_cols = con.execute(f\"\"\"\n",
    "#     SELECT \n",
    "#         contest_id,\n",
    "#         -- Add other specific columns you need\n",
    "#         load_ts\n",
    "#     FROM read_json(\n",
    "#         '{contest_analyze_path}',\n",
    "#         format='newline_delimited',\n",
    "#         compression='gzip',\n",
    "#         maximum_object_size={MAX_OBJECT_SIZE}\n",
    "#     )\n",
    "#     LIMIT 100\n",
    "# \"\"\").df()\n",
    "\n",
    "print(\"Best practice: SELECT only columns you need\")\n",
    "print(\"This reduces memory usage significantly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Filter data before loading\n",
    "# filtered = con.execute(f\"\"\"\n",
    "#     SELECT *\n",
    "#     FROM read_json(\n",
    "#         '{contest_analyze_path}',\n",
    "#         format='newline_delimited',\n",
    "#         compression='gzip',\n",
    "#         maximum_object_size={MAX_OBJECT_SIZE}\n",
    "#     )\n",
    "#     WHERE contest_id = 12345  -- Filter early!\n",
    "# \"\"\").df()\n",
    "\n",
    "print(\"Add WHERE clauses to filter BEFORE loading into pandas\")\n",
    "print(\"DuckDB will only load matching records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Aggregate without loading all data\n",
    "# stats = con.execute(f\"\"\"\n",
    "#     SELECT \n",
    "#         COUNT(*) as total_contests,\n",
    "#         COUNT(DISTINCT contest_id) as unique_contests,\n",
    "#         MIN(load_ts) as earliest_load,\n",
    "#         MAX(load_ts) as latest_load\n",
    "#     FROM read_json(\n",
    "#         '{contest_analyze_path}',\n",
    "#         format='newline_delimited',\n",
    "#         compression='gzip',\n",
    "#         maximum_object_size={MAX_OBJECT_SIZE}\n",
    "#     )\n",
    "# \"\"\").df()\n",
    "\n",
    "print(\"Aggregate in SQL - much faster and uses less memory\")\n",
    "print(\"Let DuckDB do the heavy lifting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Working with Nested Fields\n",
    "\n",
    "If your data has nested dicts/lists, here's how to access them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: JSON operators (if stored as JSON strings)\n",
    "# result = con.execute(f\"\"\"\n",
    "#     SELECT \n",
    "#         contest_id,\n",
    "#         nested_column->>'$.some_key' as extracted_value,\n",
    "#         nested_column->'$.another_key' as extracted_json\n",
    "#     FROM read_json(\n",
    "#         '{contest_analyze_path}',\n",
    "#         format='newline_delimited',\n",
    "#         compression='gzip',\n",
    "#         maximum_object_size={MAX_OBJECT_SIZE}\n",
    "#     )\n",
    "#     LIMIT 10\n",
    "# \"\"\").df()\n",
    "\n",
    "print(\"JSON operators for nested access:\")\n",
    "print(\"  column->'$.key'      -- Returns JSON\")\n",
    "print(\"  column->>'$.key'     -- Returns string\")\n",
    "print(\"  column['key']        -- For STRUCT types\")\n",
    "print(\"  column[1]            -- For LIST types (0-indexed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Load into pandas and process there\n",
    "# (Only for small filtered datasets!)\n",
    "\n",
    "# sample_df = con.execute(f\"\"\"\n",
    "#     SELECT *\n",
    "#     FROM read_json(\n",
    "#         '{contest_analyze_path}',\n",
    "#         format='newline_delimited',\n",
    "#         compression='gzip',\n",
    "#         maximum_object_size={MAX_OBJECT_SIZE}\n",
    "#     )\n",
    "#     LIMIT 1\n",
    "# \"\"\").df()\n",
    "\n",
    "# # Access nested fields in pandas\n",
    "# for col in sample_df.columns:\n",
    "#     value = sample_df[col].iloc[0]\n",
    "#     if isinstance(value, dict):\n",
    "#         print(f\"{col} keys: {list(value.keys())}\")\n",
    "\n",
    "print(\"Process nested data in pandas if needed\")\n",
    "print(\"But ONLY after filtering to small dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Check Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify load timestamp\n",
    "ts_check = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        MIN(load_ts) as earliest_load,\n",
    "        MAX(load_ts) as latest_load,\n",
    "        COUNT(*) as record_count,\n",
    "        COUNT(DISTINCT load_ts) as unique_timestamps\n",
    "    FROM read_json(\n",
    "        '{contest_analyze_path}',\n",
    "        format='newline_delimited',\n",
    "        compression='gzip',\n",
    "        maximum_object_size={MAX_OBJECT_SIZE}\n",
    "    )\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"Load timestamp verification:\")\n",
    "ts_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Process in Chunks\n",
    "\n",
    "If even filtered queries use too much memory, process data in chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Process in chunks using LIMIT/OFFSET\n",
    "chunk_size = 10\n",
    "offset = 0\n",
    "\n",
    "results = []\n",
    "\n",
    "# Get total count first\n",
    "total = con.execute(f\"\"\"\n",
    "    SELECT COUNT(*) as cnt\n",
    "    FROM read_json(\n",
    "        '{contest_analyze_path}',\n",
    "        format='newline_delimited',\n",
    "        compression='gzip',\n",
    "        maximum_object_size={MAX_OBJECT_SIZE}\n",
    "    )\n",
    "\"\"\").fetchone()[0]\n",
    "\n",
    "print(f\"Total records: {total}\")\n",
    "print(f\"Processing in chunks of {chunk_size}...\")\n",
    "\n",
    "# Process first 2 chunks as example\n",
    "for i in range(min(2, (total + chunk_size - 1) // chunk_size)):\n",
    "    chunk = con.execute(f\"\"\"\n",
    "        SELECT *\n",
    "        FROM read_json(\n",
    "            '{contest_analyze_path}',\n",
    "            format='newline_delimited',\n",
    "            compression='gzip',\n",
    "            maximum_object_size={MAX_OBJECT_SIZE}\n",
    "        )\n",
    "        LIMIT {chunk_size}\n",
    "        OFFSET {offset}\n",
    "    \"\"\").df()\n",
    "    \n",
    "    print(f\"  Chunk {i+1}: {len(chunk)} records\")\n",
    "    \n",
    "    # Process chunk here\n",
    "    # results.append(chunk)\n",
    "    \n",
    "    offset += chunk_size\n",
    "\n",
    "print(\"\\nChunked processing prevents memory overflow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "**1. Still getting \"maximum_object_size\" error:**\n",
    "- Increase `MAX_OBJECT_SIZE` further (try 128MB or 256MB)\n",
    "- Your objects might be even larger than 64MB\n",
    "\n",
    "**2. Out of memory when loading:**\n",
    "- Don't use `.df()` on large result sets\n",
    "- Add WHERE clauses to filter first\n",
    "- Use aggregations instead of loading raw data\n",
    "- Process in chunks (see example above)\n",
    "\n",
    "**3. Slow queries:**\n",
    "- JSONL is inherently slower than Parquet\n",
    "- Consider converting to Parquet for repeated analysis\n",
    "- But keep JSONL in staging for raw data preservation\n",
    "\n",
    "**4. Can't access nested fields:**\n",
    "- Check if nested data is stored as strings or native types\n",
    "- Use `type(sample[col].iloc[0])` to check\n",
    "- For strings: parse with `json.loads()` in pandas\n",
    "- For STRUCT/LIST: use DuckDB's nested operators\n",
    "\n",
    "### Memory Guidelines\n",
    "\n",
    "**Rule of thumb:**\n",
    "- Each 64MB JSON object → ~100-200MB in pandas DataFrame\n",
    "- 10 records of 64MB each → 1-2GB memory needed\n",
    "- Always filter/limit before calling `.df()`\n",
    "\n",
    "**Best practices:**\n",
    "1. Filter with WHERE in SQL\n",
    "2. Select only needed columns\n",
    "3. Aggregate in SQL when possible\n",
    "4. Process in chunks for large datasets\n",
    "5. Don't call `.df()` without LIMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**If this works well:**\n",
    "1. Use filtered queries to extract specific data you need\n",
    "2. Consider creating processed views in DDS layer\n",
    "3. Convert commonly-queried data to Parquet for speed\n",
    "\n",
    "**If still having memory issues:**\n",
    "1. Restructure contest_analyze data upstream (in scraper)\n",
    "2. Save as multiple files instead of one large file\n",
    "3. Flatten nested structures before saving\n",
    "4. Use streaming processing (chunk by chunk)\n",
    "\n",
    "**For production pipelines:**\n",
    "```python\n",
    "# Keep staging as raw JSONL\n",
    "Staging: contest_analyze/*.json.gz  (raw, nested)\n",
    "\n",
    "# Process to DDS as flattened Parquet\n",
    "DDS: contest_analyze_processed/*.parquet  (clean, flat, fast)\n",
    "\n",
    "# Aggregate to DDM for analytics\n",
    "DDM: contest_summaries/*.parquet  (aggregated, business-ready)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection\n",
    "con.close()\n",
    "print(\"✓ DuckDB connection closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
