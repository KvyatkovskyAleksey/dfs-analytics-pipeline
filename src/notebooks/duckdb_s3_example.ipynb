{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# DuckDB with S3/Wasabi Storage\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "- Configure DuckDB to work with S3/Wasabi\n",
    "- Save data to S3 in Parquet format\n",
    "- Read JSON from APIs and save to S3\n",
    "- Query data directly from S3\n",
    "- Create data marts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Setup: Configure DuckDB with S3 Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Get Wasabi credentials from environment\n",
    "wasabi_endpoint = os.getenv('WASABI_ENDPOINT', 's3.us-east-2.wasabisys.com')\n",
    "wasabi_access_key = os.getenv('WASABI_ACCESS_KEY')\n",
    "wasabi_secret_key = os.getenv('WASABI_SECRET_KEY')\n",
    "\n",
    "# Create DuckDB connection\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Configure S3 settings\n",
    "con.execute(f\"\"\"\n",
    "    SET s3_endpoint='{wasabi_endpoint}';\n",
    "    SET s3_access_key_id='{wasabi_access_key}';\n",
    "    SET s3_secret_access_key='{wasabi_secret_key}';\n",
    "    SET s3_url_style='path';\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì DuckDB configured with S3 credentials\")\n",
    "print(f\"Endpoint: {wasabi_endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Example 1: Write DataFrame to S3 as Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "sample_data = [\n",
    "    {'id': 1, 'name': 'Alice', 'age': 25, 'department': 'Engineering'},\n",
    "    {'id': 2, 'name': 'Bob', 'age': 30, 'department': 'Marketing'},\n",
    "    {'id': 3, 'name': 'Charlie', 'age': 35, 'department': 'Sales'},\n",
    "    {'id': 4, 'name': 'Diana', 'age': 28, 'department': 'Engineering'},\n",
    "    {'id': 5, 'name': 'Eve', 'age': 32, 'department': 'HR'}\n",
    "]\n",
    "\n",
    "# Create table from Python data\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE employees AS \n",
    "    SELECT * FROM (VALUES\n",
    "        (1, 'Alice', 25, 'Engineering'),\n",
    "        (2, 'Bob', 30, 'Marketing'),\n",
    "        (3, 'Charlie', 35, 'Sales'),\n",
    "        (4, 'Diana', 28, 'Engineering'),\n",
    "        (5, 'Eve', 32, 'HR')\n",
    "    ) AS t(id, name, age, department)\n",
    "\"\"\")\n",
    "\n",
    "# View the data\n",
    "con.execute(\"SELECT * FROM employees\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to S3 as Parquet\n",
    "bucket_name = \"dfscrunch-data-lake\"\n",
    "output_path = f\"s3://{bucket_name}/duckdb-examples/employees.parquet\"\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "    COPY employees \n",
    "    TO '{output_path}' \n",
    "    (FORMAT PARQUET, COMPRESSION 'SNAPPY')\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úì Data written to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Example 2: Read JSON from API and Save to S3\n",
    "\n",
    "This example shows how to:\n",
    "1. Fetch JSON data from an API\n",
    "2. Load it into DuckDB\n",
    "3. Transform it\n",
    "4. Save to S3 as Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate API response (replace with actual API call)\n",
    "# In real scenario: response = requests.get('https://api.example.com/data').json()\n",
    "\n",
    "api_response = {\n",
    "    \"users\": [\n",
    "        {\"user_id\": 101, \"username\": \"john_doe\", \"email\": \"john@example.com\", \"created_at\": \"2024-01-15\"},\n",
    "        {\"user_id\": 102, \"username\": \"jane_smith\", \"email\": \"jane@example.com\", \"created_at\": \"2024-02-20\"},\n",
    "        {\"user_id\": 103, \"username\": \"bob_jones\", \"email\": \"bob@example.com\", \"created_at\": \"2024-03-10\"},\n",
    "    ],\n",
    "    \"metadata\": {\n",
    "        \"total\": 3,\n",
    "        \"timestamp\": \"2024-10-05T20:00:00Z\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"API Response:\")\n",
    "print(json.dumps(api_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Load JSON directly into DuckDB\n",
    "# Save JSON to temporary file (or use read_json_auto for files)\n",
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:\n",
    "    json.dump(api_response['users'], f)\n",
    "    temp_json_path = f.name\n",
    "\n",
    "# Read JSON file into DuckDB\n",
    "users_df = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        user_id,\n",
    "        username,\n",
    "        email,\n",
    "        CAST(created_at AS DATE) as created_at,\n",
    "        CURRENT_TIMESTAMP as ingested_at\n",
    "    FROM read_json_auto('{temp_json_path}')\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"Transformed data:\")\n",
    "print(users_df)\n",
    "\n",
    "# Clean up temp file\n",
    "os.unlink(temp_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": "# Method 2: Load from Python object via pandas\n# Convert to pandas DataFrame first, then DuckDB can read it\n\nimport pandas as pd\n\n# Convert API response to DataFrame\nusers_df = pd.DataFrame(api_response['users'])\n\n# Create table from DataFrame\ncon.execute(\"\"\"\n    CREATE OR REPLACE TABLE api_users AS \n    SELECT \n        user_id,\n        username,\n        email,\n        CAST(created_at AS DATE) as created_at,\n        CURRENT_TIMESTAMP as ingested_at\n    FROM users_df\n\"\"\")\n\ncon.execute(\"SELECT * FROM api_users\").df()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save API data to S3 as Parquet\n",
    "api_output_path = f\"s3://{bucket_name}/duckdb-examples/api_users.parquet\"\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "    COPY api_users \n",
    "    TO '{api_output_path}' \n",
    "    (FORMAT PARQUET, COMPRESSION 'SNAPPY')\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úì API data saved to: {api_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Example 3: Query Parquet Files Directly from S3\n",
    "\n",
    "DuckDB can query Parquet files on S3 without loading them into memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query S3 data directly\n",
    "result = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        COUNT(*) as employee_count,\n",
    "        AVG(age) as avg_age,\n",
    "        MIN(age) as min_age,\n",
    "        MAX(age) as max_age\n",
    "    FROM read_parquet('s3://{bucket_name}/duckdb-examples/employees.parquet')\n",
    "    GROUP BY department\n",
    "    ORDER BY employee_count DESC\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"Department Statistics:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query multiple S3 files with wildcard\n",
    "# (useful for partitioned data)\n",
    "result = con.execute(f\"\"\"\n",
    "    SELECT COUNT(*) as total_records\n",
    "    FROM read_parquet('s3://{bucket_name}/duckdb-examples/*.parquet')\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"Total records across all files: {result['total_records'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Example 4: Create a Data Mart\n",
    "\n",
    "Combine multiple datasets to create an analytical mart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mart by joining data from S3\n",
    "mart = con.execute(f\"\"\"\n",
    "    WITH employee_summary AS (\n",
    "        SELECT \n",
    "            department,\n",
    "            COUNT(*) as total_employees,\n",
    "            AVG(age) as avg_age\n",
    "        FROM read_parquet('s3://{bucket_name}/duckdb-examples/employees.parquet')\n",
    "        GROUP BY department\n",
    "    ),\n",
    "    user_summary AS (\n",
    "        SELECT \n",
    "            COUNT(*) as total_users,\n",
    "            COUNT(DISTINCT DATE_TRUNC('month', created_at)) as active_months\n",
    "        FROM read_parquet('s3://{bucket_name}/duckdb-examples/api_users.parquet')\n",
    "    )\n",
    "    SELECT \n",
    "        e.department,\n",
    "        e.total_employees,\n",
    "        ROUND(e.avg_age, 1) as avg_age,\n",
    "        u.total_users,\n",
    "        u.active_months,\n",
    "        CURRENT_TIMESTAMP as mart_created_at\n",
    "    FROM employee_summary e\n",
    "    CROSS JOIN user_summary u\n",
    "    ORDER BY e.total_employees DESC\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"Data Mart:\")\n",
    "print(mart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": "# Save mart back to S3\nmart_output_path = f\"s3://{bucket_name}/ddm/employee_user_summary.parquet\"\n\ncon.execute(f\"\"\"\n    COPY (\n        WITH employee_summary AS (\n            SELECT \n                department,\n                COUNT(*) as total_employees,\n                AVG(age) as avg_age\n            FROM read_parquet('s3://{bucket_name}/duckdb-examples/employees.parquet')\n            GROUP BY department\n        ),\n        user_summary AS (\n            SELECT \n                COUNT(*) as total_users,\n                COUNT(DISTINCT DATE_TRUNC('month', created_at)) as active_months\n            FROM read_parquet('s3://{bucket_name}/duckdb-examples/api_users.parquet')\n        )\n        SELECT \n            e.department,\n            e.total_employees,\n            ROUND(e.avg_age, 1) as avg_age,\n            u.total_users,\n            u.active_months,\n            CURRENT_TIMESTAMP as mart_created_at\n        FROM employee_summary e\n        CROSS JOIN user_summary u\n    )\n    TO '{mart_output_path}'\n    (FORMAT PARQUET, COMPRESSION 'SNAPPY')\n\"\"\")\n\nprint(f\"‚úì Mart saved to: {mart_output_path}\")"
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Example 5: Working with Real API Data\n",
    "\n",
    "Here's a practical example with a real API (JSONPlaceholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requests if needed\n",
    "# !pip install requests\n",
    "\n",
    "import requests\n",
    "\n",
    "# Fetch data from a real API\n",
    "response = requests.get('https://jsonplaceholder.typicode.com/users')\n",
    "users_api_data = response.json()\n",
    "\n",
    "print(f\"Fetched {len(users_api_data)} users from API\")\n",
    "print(\"Sample record:\")\n",
    "print(json.dumps(users_api_data[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": "# Flatten nested JSON and load into DuckDB\nflattened_users = [\n    {\n        'user_id': u['id'],\n        'name': u['name'],\n        'username': u['username'],\n        'email': u['email'],\n        'city': u['address']['city'],\n        'company': u['company']['name'],\n    }\n    for u in users_api_data\n]\n\n# Convert to DataFrame\nflattened_df = pd.DataFrame(flattened_users)\n\n# Create table from DataFrame - DuckDB reads pandas DataFrames directly!\ncon.execute(\"\"\"\n    CREATE OR REPLACE TABLE real_api_users AS \n    SELECT \n        *,\n        CURRENT_TIMESTAMP as ingested_at\n    FROM flattened_df\n\"\"\")\n\n# View the data\ncon.execute(\"SELECT * FROM real_api_users LIMIT 5\").df()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to S3\n",
    "real_api_output = f\"s3://{bucket_name}/duckdb-examples/real_api_users.parquet\"\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "    COPY real_api_users \n",
    "    TO '{real_api_output}' \n",
    "    (FORMAT PARQUET, COMPRESSION 'SNAPPY')\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úì Real API data saved to: {real_api_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lc8qo0w56v",
   "source": "## Example 7: Schema Inspection (Like Spark's printSchema)\n\nDuckDB has multiple ways to inspect schemas - some even better than Spark!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "xfzhhbk71xc",
   "source": "# Method 1: DESCRIBE - Most like Spark's printSchema()\nprint(\"=\" * 60)\nprint(\"Method 1: DESCRIBE (like Spark's df.printSchema())\")\nprint(\"=\" * 60)\n\nschema = con.execute(\"DESCRIBE employees\").df()\nprint(schema)\nprint(\"\\nColumns:\")\nfor _, row in schema.iterrows():\n    print(f\"  {row['column_name']}: {row['column_type']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "jg4vpfop4",
   "source": "# Method 2: SUMMARIZE - Schema + Statistics (DuckDB exclusive!)\n# This is BETTER than Spark - you get schema AND data stats!\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Method 2: SUMMARIZE (schema + stats - unique to DuckDB!)\")\nprint(\"=\" * 60)\n\nsummary = con.execute(\"SUMMARIZE employees\").df()\nprint(summary.to_string())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "skkwc4x7eo",
   "source": "# Method 3: DESCRIBE for S3 Parquet files (without loading!)\n# This is AMAZING - inspect schema of S3 files without downloading!\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Method 3: DESCRIBE S3 Parquet (no download needed!)\")\nprint(\"=\" * 60)\n\ns3_schema = con.execute(f\"\"\"\n    DESCRIBE SELECT * FROM read_parquet('s3://{bucket_name}/duckdb-examples/employees.parquet')\n\"\"\").df()\n\nprint(s3_schema)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "vtmt0sznfjd",
   "source": "# Method 4: PRAGMA table_info - Detailed metadata\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Method 4: PRAGMA table_info (detailed metadata)\")\nprint(\"=\" * 60)\n\ntable_info = con.execute(\"PRAGMA table_info('employees')\").df()\nprint(table_info)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "39758bgu9rl",
   "source": "# Method 5: Python .description (programmatic access)\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Method 5: Python .description (for programmatic use)\")\nprint(\"=\" * 60)\n\nresult = con.execute(\"SELECT * FROM employees LIMIT 0\")\nprint(\"Column metadata:\")\nfor col in result.description:\n    print(f\"  Name: {col[0]}, Type: {col[1]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "dinchxfmm6n",
   "source": "# Method 6: Pandas dtypes (when working with DataFrames)\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Method 6: Pandas .dtypes (DataFrame schema)\")\nprint(\"=\" * 60)\n\ndf = con.execute(\"SELECT * FROM employees\").df()\nprint(df.dtypes)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "fy8buyrfvm8",
   "source": "# BONUS: Inspect nested Parquet schema (Staging layer example)\nprint(\"\\n\" + \"=\" * 60)\nprint(\"BONUS: Nested structure schema (Staging layer)\")\nprint(\"=\" * 60)\n\nnested_schema = con.execute(f\"\"\"\n    DESCRIBE SELECT * FROM read_parquet('{staging_path}')\n\"\"\").df()\n\nprint(nested_schema)\nprint(\"\\nNotice the STRUCT and LIST types - DuckDB preserves nested structures!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "81j15hoprax",
   "source": "### DuckDB vs Spark Schema Inspection\n\n| Feature | Spark | DuckDB |\n|---------|-------|---------|\n| **Basic schema** | `df.printSchema()` | `DESCRIBE table` ‚úì |\n| **With statistics** | `df.describe()` | `SUMMARIZE table` ‚úì‚úì Better! |\n| **S3 without loading** | ‚ùå Need to load data | `DESCRIBE SELECT * FROM read_parquet('s3://...')` ‚úì |\n| **Nested types** | ‚úì Shows | ‚úì Shows (STRUCT, LIST, MAP) |\n| **Programmatic** | `df.schema` | `.description` or `.df().dtypes` ‚úì |\n\n**Winner: DuckDB** üèÜ\n- `SUMMARIZE` gives schema + stats in one command\n- Can inspect S3 files without downloading\n- Cleaner SQL syntax",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "rw9fmv9ld8k",
   "source": "## Example 6: Raw vs Processed Data (Staging ‚Üí DDS ‚Üí DDM)\n\n**Best Practice**: Don't flatten raw data! Save it nested, then process later.\n\n### Data Lake Layers:\n- **Staging**: Save API responses as-is (nested structures, raw data)\n- **DDS** (Data Detail Store): Flatten, clean, deduplicate\n- **DDM** (Data Mart Domain): Aggregated, business-ready tables",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "21dky6dqmhe",
   "source": "# Step 1: STAGING LAYER - Save raw nested JSON to S3\n# Keep the original structure! Parquet supports nested types.\n\n# Simulate complex API response with nested data\nraw_api_response = {\n    \"data\": [\n        {\n            \"order_id\": 1001,\n            \"customer\": {\n                \"id\": 501,\n                \"name\": \"Alice Johnson\",\n                \"email\": \"alice@example.com\"\n            },\n            \"items\": [\n                {\"product\": \"Laptop\", \"quantity\": 1, \"price\": 1200.00},\n                {\"product\": \"Mouse\", \"quantity\": 2, \"price\": 25.00}\n            ],\n            \"total\": 1250.00,\n            \"order_date\": \"2024-10-01\"\n        },\n        {\n            \"order_id\": 1002,\n            \"customer\": {\n                \"id\": 502,\n                \"name\": \"Bob Smith\",\n                \"email\": \"bob@example.com\"\n            },\n            \"items\": [\n                {\"product\": \"Keyboard\", \"quantity\": 1, \"price\": 85.00}\n            ],\n            \"total\": 85.00,\n            \"order_date\": \"2024-10-02\"\n        }\n    ]\n}\n\n# Convert to DataFrame - pandas preserves nested structures!\nraw_df = pd.DataFrame(raw_api_response['data'])\n\nprint(\"Raw nested data:\")\nprint(raw_df.head())\nprint(\"\\nData types (notice dict and list types):\")\nprint(raw_df.dtypes)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "9g7cxkmflqs",
   "source": "# Save nested data to Staging layer - NO FLATTENING!\nstaging_path = f\"s3://{bucket_name}/staging/orders/raw_orders.parquet\"\n\ncon.execute(f\"\"\"\n    CREATE OR REPLACE TABLE staging_orders AS \n    SELECT \n        *,\n        CURRENT_TIMESTAMP as ingested_at\n    FROM raw_df\n\"\"\")\n\ncon.execute(f\"\"\"\n    COPY staging_orders \n    TO '{staging_path}' \n    (FORMAT PARQUET, COMPRESSION 'SNAPPY')\n\"\"\")\n\nprint(f\"‚úì Raw nested data saved to Staging layer: {staging_path}\")\nprint(\"  ‚Üí Preserved nested customer and items structures!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "78wo1kiyidd",
   "source": "# Step 2: Query nested Parquet directly from S3!\n# DuckDB can access nested fields using dot notation\n\nresult = con.execute(f\"\"\"\n    SELECT \n        order_id,\n        customer.id as customer_id,\n        customer.name as customer_name,\n        customer.email as customer_email,\n        total,\n        order_date,\n        items  -- Keep array as-is for now\n    FROM read_parquet('{staging_path}')\n\"\"\").df()\n\nprint(\"Querying nested Parquet from Staging layer:\")\nprint(result)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "vmwry79a0rd",
   "source": "# Step 3: DDS LAYER - Flatten and process\n# Now we transform nested ‚Üí flat for analytics\n\ndds_path = f\"s3://{bucket_name}/dds/orders/orders_flattened.parquet\"\n\ncon.execute(f\"\"\"\n    CREATE OR REPLACE TABLE dds_orders AS\n    SELECT \n        order_id,\n        customer.id as customer_id,\n        customer.name as customer_name,\n        customer.email as customer_email,\n        CAST(order_date AS DATE) as order_date,\n        total as order_total,\n        CAST(ingested_at AS TIMESTAMP) as ingested_at,\n        CURRENT_TIMESTAMP as processed_at\n    FROM read_parquet('{staging_path}')\n\"\"\")\n\n# Save to DDS layer\ncon.execute(f\"\"\"\n    COPY dds_orders \n    TO '{dds_path}' \n    (FORMAT PARQUET, COMPRESSION 'SNAPPY')\n\"\"\")\n\nprint(f\"‚úì Flattened data saved to DDS layer: {dds_path}\")\n\n# View processed data\ncon.execute(\"SELECT * FROM dds_orders\").df()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "2zx7o8c55mf",
   "source": "# Step 4: DDM LAYER - Create business-ready mart\n# Aggregate and create analytical tables\n\nddm_path = f\"s3://{bucket_name}/ddm/marts/customer_summary.parquet\"\n\ncon.execute(f\"\"\"\n    COPY (\n        SELECT \n            customer_id,\n            customer_name,\n            customer_email,\n            COUNT(*) as total_orders,\n            SUM(order_total) as total_revenue,\n            AVG(order_total) as avg_order_value,\n            MIN(order_date) as first_order_date,\n            MAX(order_date) as last_order_date\n        FROM read_parquet('{dds_path}')\n        GROUP BY customer_id, customer_name, customer_email\n        ORDER BY total_revenue DESC\n    )\n    TO '{ddm_path}'\n    (FORMAT PARQUET, COMPRESSION 'SNAPPY')\n\"\"\")\n\nprint(f\"‚úì Mart saved to DDM layer: {ddm_path}\")\n\n# Query the mart\nmart_result = con.execute(f\"\"\"\n    SELECT * FROM read_parquet('{ddm_path}')\n\"\"\").df()\n\nprint(\"\\nCustomer Summary Mart:\")\nprint(mart_result)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2vamgs3excm",
   "source": "### Why This Layered Approach?\n\n**Staging Layer Benefits:**\n- ‚úÖ **Preserve original data** - can always reprocess if logic changes\n- ‚úÖ **Audit trail** - know exactly what API returned\n- ‚úÖ **No data loss** - nested structures intact\n- ‚úÖ **Fast ingestion** - just save, no transformation\n- ‚úÖ **Parquet supports nested types** - no need to flatten\n\n**DDS (Data Detail Store) Layer Benefits:**\n- ‚úÖ **Clean data** - deduplication, validation\n- ‚úÖ **Flattened** - easier to query\n- ‚úÖ **Type conversions** - proper dates, numbers\n- ‚úÖ **Standardized** - consistent schema\n- ‚úÖ **Business rules applied** - enriched and validated\n\n**DDM (Data Mart Domain) Layer Benefits:**\n- ‚úÖ **Business-ready** - aggregated metrics\n- ‚úÖ **Fast queries** - pre-computed\n- ‚úÖ **Denormalized** - optimized for analytics\n- ‚úÖ **Self-service ready** - BI tools can consume directly\n\n**For your use case:**\n```\nAPI JSON ‚Üí Staging (nested Parquet) ‚Üí DDS (flat Parquet) ‚Üí DDM (marts)\n```\n\nThis is a proven data lakehouse pattern! üéØ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close DuckDB connection\n",
    "con.close()\n",
    "print(\"‚úì DuckDB connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": "## Summary\n\nThis notebook demonstrated:\n\n1. ‚úì **S3 Configuration**: Set up DuckDB with Wasabi credentials\n2. ‚úì **Write to S3**: Save DataFrames as Parquet on S3\n3. ‚úì **API to S3**: Fetch JSON from APIs and save to S3\n4. ‚úì **Query S3**: Read and analyze Parquet files directly from S3\n5. ‚úì **Create Marts**: Build analytical datasets from S3 data\n6. ‚úì **Layered Architecture**: Staging (raw) ‚Üí DDS (processed) ‚Üí DDM (marts)\n7. ‚úì **Schema Inspection**: Multiple ways to view schemas\n\n### Key Advantages of DuckDB:\n- ‚ö° **Fast** - 10-100x faster than Spark for small-to-medium data\n- üéØ **Direct S3 queries** - no need to load data into memory\n- ü™∂ **Lightweight** - runs in-process, no cluster needed\n- üêç **Pure Python** - easy to integrate with Airflow\n- üí∞ **Cost-effective** - minimal infrastructure\n- üì¶ **Nested data support** - query Parquet with nested structures\n\n### Best Practices for Data Lakehouse:\n1. **Staging layer** - Save raw data as-is, preserve original structure\n2. **DDS layer** - Process and flatten, clean and standardize\n3. **DDM layer** - Create marts, aggregate for analytics\n4. **Use Parquet** - Columnar, compressed, supports nested types\n5. **Query in-place** - DuckDB reads S3 directly, no copying needed\n\n### Architecture Pattern:\n```\nAPI (JSON) ‚Üí Staging (nested Parquet) ‚Üí DDS (flat Parquet) ‚Üí DDM (aggregated marts)\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}