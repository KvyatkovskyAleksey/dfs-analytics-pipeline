{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Loading JSONL Staging Data from S3\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "- Load gzipped JSONL files saved by `DuckDBStagingProcessor`\n",
    "- Query contest_analyze_data with nested structures\n",
    "- Inspect schemas and verify data integrity\n",
    "- Work with memory-efficient queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Configure DuckDB with S3 Credentials"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T02:09:41.091461Z",
     "start_time": "2025-10-07T02:09:40.831865Z"
    }
   },
   "source": [
    "import duckdb\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Get Wasabi credentials from environment\n",
    "wasabi_endpoint = os.getenv('WASABI_ENDPOINT', 's3.us-east-2.wasabisys.com')\n",
    "wasabi_access_key = os.getenv('WASABI_ACCESS_KEY')\n",
    "wasabi_secret_key = os.getenv('WASABI_SECRET_KEY')\n",
    "bucket_name = os.getenv('WASABI_BUCKET_NAME')\n",
    "\n",
    "# Create DuckDB connection\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Configure S3 settings\n",
    "con.execute(f\"\"\"\n",
    "    SET s3_endpoint='{wasabi_endpoint}';\n",
    "    SET s3_access_key_id='{wasabi_access_key}';\n",
    "    SET s3_secret_access_key='{wasabi_secret_key}';\n",
    "    SET s3_url_style='path';\n",
    "\"\"\")\n",
    "\n",
    "print(\"âœ“ DuckDB configured with S3 credentials\")\n",
    "print(f\"Endpoint: {wasabi_endpoint}\")\n",
    "print(f\"Bucket: {bucket_name}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ DuckDB configured with S3 credentials\n",
      "Endpoint: s3.us-east-2.wasabisys.com\n",
      "Bucket: dfscrunch-data-lake\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration: Set your sport and date\n",
    "\n",
    "Update these values to match the data you've saved:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T02:11:46.123203Z",
     "start_time": "2025-10-07T02:11:46.120452Z"
    }
   },
   "source": [
    "# Update these to match your data\n",
    "SPORT = \"NFL\"  # e.g., \"nfl\", \"nba\", etc.\n",
    "DATE = \"2025-10-02\"  # Format: YYYY-MM-DD\n",
    "GAME_TYPE = \"dk_single_game\"  # e.g., \"classic\", \"showdown\", etc.\n",
    "\n",
    "# Construct base path\n",
    "base_path = f\"s3://{bucket_name}/staging/{SPORT}/\"\n",
    "\n",
    "print(f\"Base path: {base_path}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base path: s3://dfscrunch-data-lake/staging/NFL/\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Load and Inspect contest_analyze Data\n",
    "\n",
    "This is the most important test since contest_analyze has nested/unstructured data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T02:14:55.058224Z",
     "start_time": "2025-10-07T02:14:34.961300Z"
    }
   },
   "source": [
    "# Path to contest_analyze data\n",
    "contest_analyze_path = f\"{base_path}contest_analyze/{GAME_TYPE}/{DATE}/data.json.gz\"\n",
    "\n",
    "print(f\"Loading from: {contest_analyze_path}\")\n",
    "\n",
    "# Load the data\n",
    "result = con.execute(f\"\"\"\n",
    "    SELECT * \n",
    "    FROM read_json('{contest_analyze_path}', \n",
    "                   format='newline_delimited',\n",
    "                   compression='gzip',\n",
    "                   maximum_object_size=67108864)\n",
    "    LIMIT 5\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"\\nLoaded {len(result)} rows (showing first 5)\")\n",
    "result"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from: s3://dfscrunch-data-lake/staging/NFL/contest_analyze/dk_single_game/2025-10-02/data.json.gz\n",
      "\n",
      "Loaded 1 rows (showing first 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                             contest  \\\n",
       "0  {'contestId': 182808315, 'contestName': 'NFL S...   \n",
       "\n",
       "                                             players  \\\n",
       "0  {'926:0': {'playerId': 926, 'firstName': 'Dava...   \n",
       "\n",
       "                                               users  \\\n",
       "0  {'gfriedmann': {'userId': 'gfriedmann', 'total...   \n",
       "\n",
       "                                            salaries  \\\n",
       "0  {'100': {'salaryCounts': {'49100': {'salaryCt'...   \n",
       "\n",
       "                                           flexUsage  \\\n",
       "0  {'100': {'flexCounts': {'RB': {'flexUsageCt': ...   \n",
       "\n",
       "                                            cptUsage  \\\n",
       "0  {'100': {'cptCounts': {'RB': {'cptUsageCt': 29...   \n",
       "\n",
       "                                        cptBreakdown  \\\n",
       "0  {'100': {'favorite': 56159, 'underdog': 31932,...   \n",
       "\n",
       "                                       favoriteUsage  \\\n",
       "0  {'100': {'favoriteCounts': {'2:4': {'favoriteC...   \n",
       "\n",
       "                                           homeUsage  \\\n",
       "0  {'100': {'homeCounts': {'2:4': {'homeCt': 1144...   \n",
       "\n",
       "                                           exposures  \\\n",
       "0  {'100': {'exposureCounts': {'119535:1': {'expo...   \n",
       "\n",
       "                                          teamStacks  \\\n",
       "0  {'100': {'teamStacksObject': {'119535:1:6957:0...   \n",
       "\n",
       "                                          gameStacks  \\\n",
       "0  {'100': {'gameStacksObject': {'119535:33213:69...   \n",
       "\n",
       "                                                cuts     load_ts  \n",
       "0  {'madeCutCounts': {'0': {'cutsCt': 88111, 'cut...  1759813350  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contest</th>\n",
       "      <th>players</th>\n",
       "      <th>users</th>\n",
       "      <th>salaries</th>\n",
       "      <th>flexUsage</th>\n",
       "      <th>cptUsage</th>\n",
       "      <th>cptBreakdown</th>\n",
       "      <th>favoriteUsage</th>\n",
       "      <th>homeUsage</th>\n",
       "      <th>exposures</th>\n",
       "      <th>teamStacks</th>\n",
       "      <th>gameStacks</th>\n",
       "      <th>cuts</th>\n",
       "      <th>load_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'contestId': 182808315, 'contestName': 'NFL S...</td>\n",
       "      <td>{'926:0': {'playerId': 926, 'firstName': 'Dava...</td>\n",
       "      <td>{'gfriedmann': {'userId': 'gfriedmann', 'total...</td>\n",
       "      <td>{'100': {'salaryCounts': {'49100': {'salaryCt'...</td>\n",
       "      <td>{'100': {'flexCounts': {'RB': {'flexUsageCt': ...</td>\n",
       "      <td>{'100': {'cptCounts': {'RB': {'cptUsageCt': 29...</td>\n",
       "      <td>{'100': {'favorite': 56159, 'underdog': 31932,...</td>\n",
       "      <td>{'100': {'favoriteCounts': {'2:4': {'favoriteC...</td>\n",
       "      <td>{'100': {'homeCounts': {'2:4': {'homeCt': 1144...</td>\n",
       "      <td>{'100': {'exposureCounts': {'119535:1': {'expo...</td>\n",
       "      <td>{'100': {'teamStacksObject': {'119535:1:6957:0...</td>\n",
       "      <td>{'100': {'gameStacksObject': {'119535:33213:69...</td>\n",
       "      <td>{'madeCutCounts': {'0': {'cutsCt': 88111, 'cut...</td>\n",
       "      <td>1759813350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check total record count\n",
    "count = con.execute(f\"\"\"\n",
    "    SELECT COUNT(*) as total_records\n",
    "    FROM read_json('{contest_analyze_path}',\n",
    "                   format='newline_delimited', \n",
    "                   compression='gzip')\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"Total records in contest_analyze: {count['total_records'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Inspect Schema\n",
    "\n",
    "Use DuckDB's schema inspection to see column types and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: DESCRIBE - see column types\n",
    "schema = con.execute(f\"\"\"\n",
    "    DESCRIBE \n",
    "    SELECT * FROM read_json('{contest_analyze_path}',\n",
    "                           format='newline_delimited',\n",
    "                           compression='gzip')\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"Schema of contest_analyze data:\")\n",
    "print(\"=\" * 60)\n",
    "for _, row in schema.iterrows():\n",
    "    print(f\"{row['column_name']:30s} : {row['column_type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: SUMMARIZE - schema + statistics\n",
    "summary = con.execute(f\"\"\"\n",
    "    SUMMARIZE \n",
    "    SELECT * FROM read_json('{contest_analyze_path}',\n",
    "                           format='newline_delimited',\n",
    "                           compression='gzip')\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"Summary statistics:\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Query Nested Fields\n",
    "\n",
    "If your data has nested JSON structures, here's how to access them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's see what one record looks like in detail\n",
    "sample = con.execute(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM read_json('{contest_analyze_path}',\n",
    "                   format='newline_delimited',\n",
    "                   compression='gzip')\n",
    "    LIMIT 1\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"Sample record structure:\")\n",
    "print(\"=\" * 60)\n",
    "for col in sample.columns:\n",
    "    value = sample[col].iloc[0]\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Type: {type(value)}\")\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"  Keys: {list(value.keys())[:10]}...\")  # Show first 10 keys\n",
    "    elif isinstance(value, list):\n",
    "        print(f\"  Length: {len(value)}\")\n",
    "        if len(value) > 0:\n",
    "            print(f\"  First item: {value[0]}\")\n",
    "    else:\n",
    "        print(f\"  Value: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Access nested fields with JSON operators\n",
    "# Adjust column names based on your actual schema\n",
    "\n",
    "# If you have nested JSON columns, use ->> or -> operators:\n",
    "# nested_query = con.execute(f\"\"\"\n",
    "#     SELECT \n",
    "#         id,\n",
    "#         nested_column->>'$.some_key' as extracted_value\n",
    "#     FROM read_json('{contest_analyze_path}',\n",
    "#                    format='newline_delimited',\n",
    "#                    compression='gzip')\n",
    "#     LIMIT 10\n",
    "# \"\"\").df()\n",
    "# nested_query\n",
    "\n",
    "print(\"For nested JSON access, use DuckDB's JSON operators:\")\n",
    "print(\"  column->'$.key'      -- Returns JSON\")\n",
    "print(\"  column->>'$.key'     -- Returns string\")\n",
    "print(\"  column['key']        -- For STRUCT types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Load Other Datasets\n",
    "\n",
    "Test loading contests, events, draft_groups, and lineups data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load contests data\n",
    "contests_path = f\"{base_path}contests/{GAME_TYPE}/{DATE}/data.json.gz\"\n",
    "\n",
    "contests = con.execute(f\"\"\"\n",
    "    SELECT * \n",
    "    FROM read_json('{contests_path}',\n",
    "                   format='newline_delimited',\n",
    "                   compression='gzip')\n",
    "    LIMIT 5\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"Contests data ({len(contests)} rows shown):\")\n",
    "contests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load events data\n",
    "events_path = f\"{base_path}events/{GAME_TYPE}/{DATE}/data.json.gz\"\n",
    "\n",
    "events = con.execute(f\"\"\"\n",
    "    SELECT * \n",
    "    FROM read_json('{events_path}',\n",
    "                   format='newline_delimited',\n",
    "                   compression='gzip')\n",
    "    LIMIT 5\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"Events data ({len(events)} rows shown):\")\n",
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load draft_groups data\n",
    "draft_groups_path = f\"{base_path}draft_groups/{DATE}/data.json.gz\"\n",
    "\n",
    "draft_groups = con.execute(f\"\"\"\n",
    "    SELECT * \n",
    "    FROM read_json('{draft_groups_path}',\n",
    "                   format='newline_delimited',\n",
    "                   compression='gzip')\n",
    "    LIMIT 5\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"Draft Groups data ({len(draft_groups)} rows shown):\")\n",
    "draft_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lineups data\n",
    "lineups_path = f\"{base_path}lineups/{GAME_TYPE}/{DATE}/data.json.gz\"\n",
    "\n",
    "lineups = con.execute(f\"\"\"\n",
    "    SELECT * \n",
    "    FROM read_json('{lineups_path}',\n",
    "                   format='newline_delimited',\n",
    "                   compression='gzip')\n",
    "    LIMIT 5\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"Lineups data ({len(lineups)} rows shown):\")\n",
    "lineups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Verify load_ts Timestamp\n",
    "\n",
    "Check that the timestamp was added correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check load_ts across all datasets\n",
    "timestamps = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        'contest_analyze' as dataset,\n",
    "        MIN(load_ts) as min_timestamp,\n",
    "        MAX(load_ts) as max_timestamp,\n",
    "        COUNT(*) as record_count\n",
    "    FROM read_json('{contest_analyze_path}',\n",
    "                   format='newline_delimited',\n",
    "                   compression='gzip')\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'contests' as dataset,\n",
    "        MIN(load_ts) as min_timestamp,\n",
    "        MAX(load_ts) as max_timestamp,\n",
    "        COUNT(*) as record_count\n",
    "    FROM read_json('{contests_path}',\n",
    "                   format='newline_delimited',\n",
    "                   compression='gzip')\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"Timestamp verification:\")\n",
    "timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Query Across Multiple Dates\n",
    "\n",
    "Use wildcards to query multiple date partitions at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query all contest_analyze data for this game type (all dates)\n",
    "all_dates_path = f\"{base_path}contest_analyze/{GAME_TYPE}/*/data.json.gz\"\n",
    "\n",
    "all_data_count = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_records,\n",
    "        COUNT(DISTINCT load_ts) as unique_load_batches\n",
    "    FROM read_json('{all_dates_path}',\n",
    "                   format='newline_delimited',\n",
    "                   compression='gzip')\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"Querying all dates with wildcard: {all_dates_path}\")\n",
    "print(all_data_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 7: Memory-Efficient Filtering\n",
    "\n",
    "DuckDB only loads the data it needs - add filters to keep memory low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Filter before loading into memory\n",
    "# Adjust column names based on your schema\n",
    "\n",
    "# filtered = con.execute(f\"\"\"\n",
    "#     SELECT *\n",
    "#     FROM read_json('{contest_analyze_path}',\n",
    "#                    format='newline_delimited',\n",
    "#                    compression='gzip')\n",
    "#     WHERE contest_id = 12345  -- Only load matching records\n",
    "#     LIMIT 100\n",
    "# \"\"\").df()\n",
    "\n",
    "print(\"Best practice: Add WHERE clauses to filter data BEFORE loading into pandas\")\n",
    "print(\"This keeps memory usage low, even with huge S3 files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 8: Convert to Parquet for Faster Queries (Optional)\n",
    "\n",
    "If you query the same data often, convert JSONL to Parquet for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Convert JSONL to Parquet for faster subsequent queries\n",
    "# parquet_path = f\"{base_path}contest_analyze_parquet/{GAME_TYPE}/{DATE}/data.parquet\"\n",
    "\n",
    "# con.execute(f\"\"\"\n",
    "#     COPY (\n",
    "#         SELECT * \n",
    "#         FROM read_json('{contest_analyze_path}',\n",
    "#                        format='newline_delimited',\n",
    "#                        compression='gzip')\n",
    "#     )\n",
    "#     TO '{parquet_path}'\n",
    "#     (FORMAT PARQUET, COMPRESSION 'SNAPPY')\n",
    "# \"\"\")\n",
    "\n",
    "# print(f\"Converted to Parquet: {parquet_path}\")\n",
    "# print(\"Parquet queries are 5-10x faster than JSONL!\")\n",
    "\n",
    "print(\"Parquet conversion commented out - uncomment to use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "Common issues and solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "Common Issues & Solutions:\n",
    "========================\n",
    "\n",
    "1. \"No files found\" error:\n",
    "   - Check SPORT, DATE, GAME_TYPE variables\n",
    "   - Verify bucket_name is correct\n",
    "   - Check S3 credentials in .env\n",
    "\n",
    "2. \"Out of memory\" when loading:\n",
    "   - Add WHERE clause to filter first\n",
    "   - Use LIMIT to test with small samples\n",
    "   - Don't call .df() on huge results\n",
    "\n",
    "3. Nested JSON showing as strings:\n",
    "   - This is expected! Pandas saved nested dicts as JSON strings\n",
    "   - Use DuckDB's JSON operators to extract:\n",
    "     - column->>'$.key' for string values\n",
    "     - column->'$.key' for JSON values\n",
    "\n",
    "4. Slow queries:\n",
    "   - JSONL is slower than Parquet\n",
    "   - Consider converting to Parquet (see Example 8)\n",
    "   - Add indexes if querying same filters often\n",
    "\n",
    "5. Schema detection issues:\n",
    "   - DuckDB auto-detects from first few rows\n",
    "   - If inconsistent, specify schema manually:\n",
    "     read_json(..., columns={'col1': 'VARCHAR', 'col2': 'INTEGER'})\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "âœ… **Successfully demonstrated:**\n",
    "1. Loading JSONL gzipped files from S3\n",
    "2. Inspecting schemas and data structure\n",
    "3. Querying nested/unstructured data\n",
    "4. Working with multiple datasets\n",
    "5. Memory-efficient filtering\n",
    "6. Querying across date partitions\n",
    "\n",
    "**Key takeaways:**\n",
    "- DuckDB reads JSONL gzipped files natively âœ“\n",
    "- No need to load everything into memory âœ“\n",
    "- Use WHERE clauses for memory efficiency âœ“\n",
    "- Nested data preserved and queryable âœ“\n",
    "- Same S3 path structure as before âœ“\n",
    "\n",
    "**Your staging pipeline is working correctly!** ðŸŽ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection\n",
    "con.close()\n",
    "print(\"âœ“ DuckDB connection closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
