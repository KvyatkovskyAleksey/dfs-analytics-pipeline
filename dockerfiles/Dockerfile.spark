FROM spark:4.0.0-scala2.13-java21-python3-ubuntu

USER root

# uv installation
# The installer requires curl (and certificates) to download the release archive
RUN apt-get update && apt-get install -y --no-install-recommends curl ca-certificates

# Download the latest installer
ADD https://astral.sh/uv/install.sh /uv-installer.sh

# Run the installer then remove it
RUN sh /uv-installer.sh && rm /uv-installer.sh

# Ensure the installed binary is on the `PATH`
ENV PATH="/root/.local/bin/:$PATH"

# Download JARs for S3/Wasabi support (Spark 4.0 Docker uses Hadoop 3.3.x with AWS SDK V1)
RUN curl -L -o /opt/spark/jars/hadoop-aws-3.3.6.jar \
      https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.6/hadoop-aws-3.3.6.jar && \
    curl -L -o /opt/spark/jars/aws-java-sdk-bundle-1.12.367.jar \
      https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.367/aws-java-sdk-bundle-1.12.367.jar

# Create a virtualenv in /opt and install dependencies using uv
WORKDIR /opt
COPY pyproject.toml ./
RUN uv sync

# Create notebooks directory and set permissions for the non-root user 'spark'
RUN mkdir -p /opt/notebooks && \
    chown -R spark:spark /opt/spark-apps /opt/notebooks /opt/.venv

# Switch to the non-root spark user
USER spark

# Add the virtualenv to the PATH. This makes `python`, `pip`, and `jupyter`
# from the virtualenv available directly.
ENV VIRTUAL_ENV=/opt/.venv
ENV PATH="/opt/.venv/bin:$PATH"
